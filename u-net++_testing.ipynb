{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8c024913","cell_type":"markdown","source":"# U-Net++ Change Detection Training Notebook\nThis notebook trains a U-Net++ model for change detection using A/B/label folders for train, val, and test. It includes installation, data loading, training with early stopping, saving metrics, and displaying results.","metadata":{}},{"id":"e4186f53","cell_type":"markdown","source":"## Assignment Compliance (Segmentation)\n- Problem: Change detection (binary segmentation of change mask)\n- Model: U-Net++ (recent variant, deeper variant over base U-Net)\n- Epochs: Min 50 with early stopping (patience 10)\n- Data: Using existing train / val / test folders exactly as provided (no re-splitting enforced).\n- Metrics tracked: IoU, Dice, Precision, Recall, F1, Accuracy, Loss + confusion matrix (pixel-wise)\n- Outputs: Metric plots, sample predictions, parameter count, GFLOPs, saved best weights.\n- Saved artifacts: best_model.pth, training_history.csv, test_metrics.csv, confusion_matrix.txt, prediction PNGs.\n","metadata":{}},{"id":"602e44af","cell_type":"code","source":"# Install all required packages\n!pip install segmentation-models-pytorch torch torchvision albumentations scikit-learn pandas tqdm thop torchinfo matplotlib seaborn --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:46:30.957160Z","iopub.execute_input":"2025-08-15T09:46:30.957413Z","iopub.status.idle":"2025-08-15T09:47:57.529266Z","shell.execute_reply.started":"2025-08-15T09:46:30.957393Z","shell.execute_reply":"2025-08-15T09:47:57.528566Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"id":"c9d4579f","cell_type":"code","source":"# Import Required Libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport segmentation_models_pytorch as smp\nfrom sklearn.metrics import f1_score, accuracy_score, jaccard_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:47:57.531515Z","iopub.execute_input":"2025-08-15T09:47:57.531721Z","iopub.status.idle":"2025-08-15T09:48:13.365759Z","shell.execute_reply.started":"2025-08-15T09:47:57.531701Z","shell.execute_reply":"2025-08-15T09:48:13.365081Z"}},"outputs":[],"execution_count":2},{"id":"1713601c","cell_type":"code","source":"# Data Loading and Dataset Definition\nDATA_ROOT = '/kaggle/input/earthquakedataset/earthquakeDataset'  # Change to your dataset path\nTRAIN_DIR = os.path.join(DATA_ROOT, 'train')\nVAL_DIR = os.path.join(DATA_ROOT, 'val')\nTEST_DIR = os.path.join(DATA_ROOT, 'test')\n\nA_TRAIN = os.path.join(TRAIN_DIR, 'A_train_aug')\nB_TRAIN = os.path.join(TRAIN_DIR, 'B_train_aug')\nLABEL_TRAIN = os.path.join(TRAIN_DIR, 'label_train_aug')\n\nA_VAL = os.path.join(VAL_DIR, 'A_val')\nB_VAL = os.path.join(VAL_DIR, 'B_val')\nLABEL_VAL = os.path.join(VAL_DIR, 'label_val')\n\nA_TEST = os.path.join(TEST_DIR, 'A_test')\nB_TEST = os.path.join(TEST_DIR, 'B_test')\nLABEL_TEST = os.path.join(TEST_DIR, 'label_test')\n\nIMG_SIZE = (256, 256)\nBATCH_SIZE = 16\nMIN_EPOCHS = 100\nPATIENCE = 10\n\nclass ChangeDataset(Dataset):\n    def __init__(self, a_dir, b_dir, label_dir):\n        self.a_files = sorted([os.path.join(a_dir, f) for f in os.listdir(a_dir) if f.endswith('.png')])\n        self.b_files = sorted([os.path.join(b_dir, f) for f in os.listdir(b_dir) if f.endswith('.png')])\n        self.label_files = sorted([os.path.join(label_dir, f) for f in os.listdir(label_dir) if f.endswith('.png')])\n        assert len(self.a_files) == len(self.b_files) == len(self.label_files)\n        \n        # Add ImageNet normalization constants\n        self.imagenet_mean = np.array([0.485, 0.456, 0.406])\n        self.imagenet_std = np.array([0.229, 0.224, 0.225])\n\n    def __len__(self):\n        return len(self.a_files)\n\n    def __getitem__(self, idx):\n        a_img = np.array(Image.open(self.a_files[idx]).convert('RGB')).astype('float32') / 255.0\n        b_img = np.array(Image.open(self.b_files[idx]).convert('RGB')).astype('float32') / 255.0\n        \n        # Apply ImageNet normalization (same as your augmentation script)\n        a_img = (a_img - self.imagenet_mean) / self.imagenet_std\n        b_img = (b_img - self.imagenet_mean) / self.imagenet_std\n        \n        label = np.array(Image.open(self.label_files[idx]).convert('L')).astype('float32') / 255.0\n        label = (label > 0.5).astype('float32')  # Fixed HTML entity\n        x = np.concatenate([a_img, b_img], axis=2)\n        x = np.transpose(x, (2, 0, 1))\n        y = label[np.newaxis, ...]\n        return torch.tensor(x), torch.tensor(y)\n\n\ntrain_ds = ChangeDataset(A_TRAIN, B_TRAIN, LABEL_TRAIN)\nval_ds = ChangeDataset(A_VAL, B_VAL, LABEL_VAL)\ntest_ds = ChangeDataset(A_TEST, B_TEST, LABEL_TEST)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:48:13.366447Z","iopub.execute_input":"2025-08-15T09:48:13.366850Z","iopub.status.idle":"2025-08-15T09:48:13.599200Z","shell.execute_reply.started":"2025-08-15T09:48:13.366817Z","shell.execute_reply":"2025-08-15T09:48:13.598347Z"}},"outputs":[],"execution_count":3},{"id":"4c606b92","cell_type":"code","source":"# Model Setup and Training Loop with extended metrics\nmodel = smp.UnetPlusPlus(\n    encoder_name='resnet34',\n    in_channels=6,\n    classes=1,\n    activation=None\n).cuda()\n\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef compute_seg_metrics(y_true, y_pred):\n    # y_true, y_pred: (N,1,H,W) float tensors\n    y_true = y_true.detach().cpu().numpy().astype('uint8')\n    y_pred = (y_pred.detach().cpu().numpy() > 0.5).astype('uint8')\n    flat_t = y_true.reshape(-1)\n    flat_p = y_pred.reshape(-1)\n    intersection = (flat_t & flat_p).sum()\n    union = (flat_t | flat_p).sum() + 1e-7\n    iou = intersection / union\n    dice = (2*intersection) / (flat_t.sum() + flat_p.sum() + 1e-7)\n    prec = precision_score(flat_t, flat_p, zero_division=0)\n    rec = recall_score(flat_t, flat_p, zero_division=0)\n    f1 = f1_score(flat_t, flat_p, zero_division=0)\n    acc = (flat_t == flat_p).mean()\n    return dict(IoU=iou, Dice=dice, Precision=prec, Recall=rec, F1=f1, Accuracy=acc)\n\nbest_val_loss = float('inf')\nepochs_no_improve = 0\nhistory = []\nfor epoch in range(1, 1000):\n    model.train()\n    train_loss = 0\n    for x, y in train_loader:\n        x, y = x.cuda(), y.cuda()\n        optimizer.zero_grad()\n        out = model(x)\n        loss = loss_fn(out, y)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * x.size(0)\n    train_loss /= len(train_loader.dataset)\n\n    model.eval()\n    val_loss = 0\n    val_preds = []\n    val_trues = []\n    with torch.no_grad():\n        for x, y in val_loader:\n            x, y = x.cuda(), y.cuda()\n            out = model(x)\n            loss = loss_fn(out, y)\n            val_loss += loss.item() * x.size(0)\n            val_preds.append(torch.sigmoid(out))\n            val_trues.append(y)\n    val_loss /= len(val_loader.dataset)\n\n    # Metrics on concatenated tensors\n    val_preds_cat = torch.cat(val_preds, dim=0)\n    val_trues_cat = torch.cat(val_trues, dim=0)\n    metrics = compute_seg_metrics(val_trues_cat, val_preds_cat)\n    history.append({'epoch':epoch,'train_loss':train_loss,'val_loss':val_loss, **metrics})\n\n    print(f\"Epoch {epoch}: TL {train_loss:.4f} VL {val_loss:.4f} IoU {metrics['IoU']:.4f} Dice {metrics['Dice']:.4f} F1 {metrics['F1']:.4f}\")\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        epochs_no_improve = 0\n        torch.save(model.state_dict(), 'best_model.pth')\n    else:\n        epochs_no_improve += 1\n\n    if epochs_no_improve >= 10:\n        print(f'Early stopping at epoch {epoch}')\n        break\n\nimport pandas as pd\nhist_df = pd.DataFrame(history)\nhist_df.to_csv('training_history.csv', index=False)\nprint('Training complete. Best val loss:', best_val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T09:48:13.600152Z","iopub.execute_input":"2025-08-15T09:48:13.600453Z","iopub.status.idle":"2025-08-15T10:06:51.470694Z","shell.execute_reply.started":"2025-08-15T09:48:13.600429Z","shell.execute_reply":"2025-08-15T10:06:51.469519Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"299b220c14784cfa8a85d375578e5385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfeb3ebd6e354679b59add6ba6d3a63f"}},"metadata":{}},{"name":"stdout","text":"Epoch 1: TL 0.3278 VL 0.2847 IoU 0.0973 Dice 0.1774 F1 0.1774\nEpoch 2: TL 0.3066 VL 0.3018 IoU 0.0000 Dice 0.0000 F1 0.0000\nEpoch 3: TL 0.3004 VL 0.2831 IoU 0.0010 Dice 0.0020 F1 0.0020\nEpoch 4: TL 0.2961 VL 0.2628 IoU 0.0228 Dice 0.0445 F1 0.0445\nEpoch 5: TL 0.2896 VL 0.4019 IoU 0.0005 Dice 0.0009 F1 0.0009\nEpoch 6: TL 0.2865 VL 0.2515 IoU 0.0941 Dice 0.1720 F1 0.1720\nEpoch 7: TL 0.2805 VL 0.2498 IoU 0.3444 Dice 0.5124 F1 0.5124\nEpoch 8: TL 0.2744 VL 0.2291 IoU 0.3562 Dice 0.5253 F1 0.5253\nEpoch 9: TL 0.2670 VL 0.2502 IoU 0.2010 Dice 0.3347 F1 0.3347\nEpoch 10: TL 0.2579 VL 0.2532 IoU 0.2951 Dice 0.4557 F1 0.4557\nEpoch 11: TL 0.2463 VL 0.2450 IoU 0.2943 Dice 0.4548 F1 0.4548\nEpoch 12: TL 0.2343 VL 0.2479 IoU 0.3784 Dice 0.5490 F1 0.5490\nEpoch 13: TL 0.2171 VL 0.2478 IoU 0.3435 Dice 0.5114 F1 0.5114\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1793392772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4},{"id":"e415ee87","cell_type":"code","source":"# Load Best Model and Evaluate on Test Set with full metrics\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom torchinfo import summary\nfrom thop import profile\n\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\n# Parameter count & GFLOPs\nsample_input = torch.randn(1,6,256,256).cuda()\nmacs, params = profile(model, inputs=(sample_input,), verbose=False)\nGFLOPs = macs/1e9\nparam_millions = params/1e6\nprint(f'GFLOPs: {GFLOPs:.3f}, Params (M): {param_millions:.3f}')\n\nall_preds = []\nall_logits = []\nall_labels = []\nwith torch.no_grad():\n    for x, y in test_loader:\n        x = x.cuda(); y = y.cuda()\n        out = model(x)\n        probs = torch.sigmoid(out)\n        preds = (probs > 0.5).float()\n        all_preds.append(preds.cpu())\n        all_logits.append(probs.cpu())\n        all_labels.append(y.cpu())\n\nall_preds = torch.cat(all_preds, dim=0)\nall_labels = torch.cat(all_labels, dim=0)\n\n# Metrics\nflat_p = all_preds.numpy().reshape(-1).astype('uint8')\nflat_t = all_labels.numpy().reshape(-1).astype('uint8')\nintersection = (flat_p & flat_t).sum()\nunion = (flat_p | flat_t).sum() + 1e-7\niou = intersection/union\ndice = (2*intersection)/(flat_p.sum()+flat_t.sum()+1e-7)\nprec = precision_score(flat_t, flat_p, zero_division=0)\nrec = recall_score(flat_t, flat_p, zero_division=0)\nf1 = f1_score(flat_t, flat_p, zero_division=0)\nacc = (flat_p==flat_t).mean()\ncm = confusion_matrix(flat_t, flat_p).astype(int)\nprint('Confusion Matrix:\\n', cm)\nmetrics = {'IoU':iou,'Dice':dice,'Precision':prec,'Recall':rec,'F1':f1,'Accuracy':acc,'GFLOPs':GFLOPs,'Params_M':param_millions}\nprint('Test Metrics:', metrics)\n\npd.DataFrame([metrics]).to_csv('test_metrics.csv', index=False)\nnp.savetxt('confusion_matrix.txt', cm, fmt='%d')\n\n# Save predictions as images (first 10)\nimport os\nos.makedirs('test_predictions', exist_ok=True)\nimport numpy as np\nfrom PIL import Image\nfor i in range(min(10, all_preds.shape[0])):\n    pred_img = (all_preds[i,0].numpy()*255).astype('uint8')\n    Image.fromarray(pred_img).save(f'test_predictions/pred_{i}.png')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T10:06:51.471422Z","iopub.status.idle":"2025-08-15T10:06:51.471640Z","shell.execute_reply.started":"2025-08-15T10:06:51.471537Z","shell.execute_reply":"2025-08-15T10:06:51.471546Z"}},"outputs":[],"execution_count":null},{"id":"416e3172","cell_type":"code","source":"# Display Final Results & Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nhist_df = pd.read_csv('training_history.csv')\nprint('History head:')\nprint(hist_df.head())\n\nfig, axes = plt.subplots(2,3, figsize=(16,8))\naxes = axes.ravel()\nplot_cols = ['train_loss','val_loss','IoU','Dice','Precision','Recall']\nfor ax,col in zip(axes, plot_cols):\n    ax.plot(hist_df['epoch'], hist_df[col], label=col)\n    ax.set_title(col)\n    ax.set_xlabel('Epoch')\n    ax.legend()\nplt.tight_layout()\nplt.show()\n\n# F1 & Accuracy separate\nplt.figure(figsize=(6,4))\nplt.plot(hist_df['epoch'], hist_df['F1'], label='F1')\nplt.plot(hist_df['epoch'], hist_df['Accuracy'], label='Accuracy')\nplt.legend(); plt.title('F1 & Accuracy'); plt.xlabel('Epoch'); plt.show()\n\n# Load test metrics\nmetrics_df = pd.read_csv('test_metrics.csv')\nprint('Test Metrics:')\nprint(metrics_df)\n\n# Confusion matrix heatmap\nimport numpy as np\ncm = np.loadtxt('confusion_matrix.txt', dtype=int)\nplt.figure(figsize=(4,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title('Pixel Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\n\n# Show sample predictions\nimport os\nfrom PIL import Image\nfor i in range(3):\n    p_path = f'test_predictions/pred_{i}.png'\n    if os.path.exists(p_path):\n        plt.figure()\n        plt.imshow(Image.open(p_path), cmap='gray')\n        plt.title(f'Pred {i}')\n        plt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T10:06:51.472843Z","iopub.status.idle":"2025-08-15T10:06:51.473106Z","shell.execute_reply.started":"2025-08-15T10:06:51.472974Z","shell.execute_reply":"2025-08-15T10:06:51.472984Z"}},"outputs":[],"execution_count":null}]}