{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# === Cell 1: Installs ===\n!pip install torch torchvision albumentations scikit-learn pandas tqdm thop torchinfo matplotlib seaborn opencv-python --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 2: Imports, device, seed, helpers ===\nimport os, random, time, math, copy\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision.models.detection import MaskRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import MultiScaleRoIAlign\nfrom torchvision.ops import FeaturePyramidNetwork\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom thop import profile\nfrom torchinfo import summary\n\n# Device & reproducibility\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nprint(\"Device:\", DEVICE)\n\n# EarlyStopping (same logic as your UNet++)\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=1e-4, restore_best=True, min_epochs=10):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best = restore_best\n        self.best_loss = None\n        self.counter = 0\n        self.best_state = None\n        self.min_epochs = min_epochs\n\n    def __call__(self, epoch, current_loss, model):\n        improved = (self.best_loss is None) or ((self.best_loss - current_loss) > self.min_delta)\n        if improved:\n            self.best_loss = current_loss\n            self.counter = 0\n            if self.restore_best:\n                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n        else:\n            self.counter += 1\n\n        if (epoch + 1) < self.min_epochs:\n            return False\n\n        if self.counter >= self.patience:\n            if self.restore_best and self.best_state is not None:\n                model.load_state_dict(self.best_state)\n            return True\n        return False\n\n# Metric helpers (union predicted instances -> semantic mask; compute IoU/Dice/...)\n@torch.no_grad()\ndef _to_binary_semantic_from_instances(outputs, image_size, score_thresh=0.5, mask_thresh=0.3):\n    if outputs is None or len(outputs.get(\"scores\", [])) == 0:\n        return torch.zeros(image_size, dtype=torch.uint8)\n    keep = outputs[\"scores\"] >= score_thresh\n    if keep.sum() == 0:\n        return torch.zeros(image_size, dtype=torch.uint8)\n    masks = outputs[\"masks\"][keep]  # (K,1,H,W)\n    bin_inst = (masks.squeeze(1) >= mask_thresh).to(torch.uint8)\n    union = torch.any(bin_inst.bool(), dim=0).to(torch.uint8)\n    return union\n\n@torch.no_grad()\ndef batch_metrics_from_outputs(outputs_list, targets_list, score_thresh=0.5, mask_thresh=0.3):\n    eps = 1e-8\n    agg = dict(tp=0, fp=0, fn=0, tn=0)\n    for outputs, target in zip(outputs_list, targets_list):\n        # target -> binary semantic\n        if isinstance(target, dict) and \"masks\" in target:\n            if len(target[\"masks\"]) == 0:\n                # infer shape from outputs if possible\n                if \"masks\" in outputs and outputs[\"masks\"].numel() > 0:\n                    tgt = torch.zeros_like(outputs[\"masks\"][0,0]).to(torch.uint8)\n                else:\n                    # fallback zero (unknown size) -> skip\n                    continue\n            else:\n                t_inst = target[\"masks\"].to(torch.uint8)\n                tgt = torch.any(t_inst.bool(), dim=0).to(torch.uint8)\n        else:\n            t = target\n            if isinstance(t, torch.Tensor):\n                if t.ndim == 3 and t.shape[0] == 1:\n                    t = t[0]\n                tgt = (t > 0).to(torch.uint8)\n            else:\n                continue\n        H, W = tgt.shape[-2], tgt.shape[-1]\n        pred = _to_binary_semantic_from_instances(outputs, (H, W), score_thresh, mask_thresh)\n        p = pred.view(-1).cpu().numpy()\n        tt = tgt.view(-1).cpu().numpy()\n        tp = int(((p == 1) & (tt == 1)).sum())\n        fp = int(((p == 1) & (tt == 0)).sum())\n        fn = int(((p == 0) & (tt == 1)).sum())\n        tn = int(((p == 0) & (tt == 0)).sum())\n        agg[\"tp\"] += tp; agg[\"fp\"] += fp; agg[\"fn\"] += fn; agg[\"tn\"] += tn\n\n    tp, fp, fn, tn = agg[\"tp\"], agg[\"fp\"], agg[\"fn\"], agg[\"tn\"]\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2*tp) / (2*tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps) if (tp+fp)>0 else 0.0\n    recall = tp / (tp + fn + eps) if (tp+fn)>0 else 0.0\n    f1 = 2*precision*recall/(precision+recall+eps) if (precision+recall)>0 else 0.0\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    return dict(tp=int(tp), fp=int(fp), fn=int(fn), tn=int(tn),\n                iou=float(iou), dice=float(dice),\n                precision=float(precision), recall=float(recall),\n                f1=float(f1), acc=float(acc))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 3: Paths, params, ImageNet normalization ===\nDATA_ROOT = '/kaggle/input/earthquakedatasetnew/earthquakeDataset'  # change if needed\nIMG_SIZE = (256,256)   # (W,H)\nTRAIN_BATCH = 6\nVAL_BATCH = 2\nTEST_BATCH = 1\nNUM_CLASSES = 2  # background + damage\n\n# Input mode: we'll use post-only RGB (ImageNet normalization)\nINPUT_MODE = 'post'\n\nIMAGE_NET_MEAN = [0.485, 0.456, 0.406]\nIMAGE_NET_STD  = [0.229, 0.224, 0.225]\n\ndef pil_to_tensor_normalized(img_pil):\n    img = np.array(img_pil).astype(np.float32) / 255.0\n    img = (img - np.array(IMAGE_NET_MEAN)) / np.array(IMAGE_NET_STD)\n    img = torch.from_numpy(img.transpose(2,0,1)).float()\n    return img","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 4: Dataset that converts semantic mask -> instances (connected components) ===\nfrom scipy import ndimage\n\nclass MaskRCNNDataset(Dataset):\n    def __init__(self, root, split='train', input_mode='post', img_size=(256,256)):\n        self.input_mode = input_mode\n        self.img_size = img_size\n        if split=='train':\n            a_dir = os.path.join(root,'train','A_train_aug')\n            b_dir = os.path.join(root,'train','B_train_aug')\n            m_dir = os.path.join(root,'train','label_train_aug')\n        elif split=='val':\n            a_dir = os.path.join(root,'val','A_val')\n            b_dir = os.path.join(root,'val','B_val')\n            m_dir = os.path.join(root,'val','label_val')\n        else:\n            a_dir = os.path.join(root,'test','A_test')\n            b_dir = os.path.join(root,'test','B_test')\n            m_dir = os.path.join(root,'test','label_test')\n        self.a_files = sorted([f for f in os.listdir(a_dir) if f.endswith('.png')])\n        self.a_dir, self.b_dir, self.m_dir = a_dir, b_dir, m_dir\n\n    def __len__(self):\n        return len(self.a_files)\n\n    def _make_input(self, a_pil, b_pil):\n        a = a_pil.resize(self.img_size, resample=Image.BILINEAR)\n        b = b_pil.resize(self.img_size, resample=Image.BILINEAR)\n        if self.input_mode == 'post':\n            return pil_to_tensor_normalized(b)\n        elif self.input_mode == 'diff':\n            a_np = np.array(a).astype(np.float32)/255.0\n            b_np = np.array(b).astype(np.float32)/255.0\n            diff = np.abs(b_np - a_np)\n            # three channels: post_R, post_G, avg_abs_diff\n            ch0 = b_np[:,:,0]; ch1 = b_np[:,:,1]; ch2 = diff.mean(axis=2)\n            stacked = np.stack([ch0, ch1, ch2], axis=2)\n            stacked = (stacked - np.array([0.485,0.456,0.485])) / np.array([0.229,0.224,0.229])\n            return torch.from_numpy(stacked.transpose(2,0,1)).float()\n        else:\n            raise ValueError(\"Unsupported input_mode\")\n\n    def _masks_to_instances(self, mask_pil):\n        mask_np = np.array(mask_pil.resize(self.img_size, resample=Image.NEAREST))\n        mask_bin = (mask_np > 127).astype(np.uint8)\n        labeled, ncomp = ndimage.label(mask_bin)\n        if ncomp == 0:\n            return [], []\n        masks = []\n        boxes = []\n        for inst_id in range(1, ncomp+1):\n            inst_mask = (labeled == inst_id).astype(np.uint8)\n            ys, xs = np.where(inst_mask)\n            if ys.size == 0 or xs.size == 0:\n                continue\n            y1, x1, y2, x2 = ys.min(), xs.min(), ys.max(), xs.max()\n            # boxes in (x1,y1,x2,y2) order\n            boxes.append([int(x1), int(y1), int(x2), int(y2)])\n            masks.append(inst_mask)\n        return masks, boxes\n\n    def __getitem__(self, idx):\n        name = self.a_files[idx]\n        a_pil = Image.open(os.path.join(self.a_dir, name)).convert('RGB')\n        b_pil = Image.open(os.path.join(self.b_dir, name)).convert('RGB')\n        m_pil = Image.open(os.path.join(self.m_dir, name)).convert('L')\n        image = self._make_input(a_pil, b_pil)  # 3,H,W\n\n        masks, boxes = self._masks_to_instances(m_pil)\n        target = {}\n        if len(masks) == 0:\n            target['boxes'] = torch.zeros((0,4), dtype=torch.float32)\n            target['labels'] = torch.zeros((0,), dtype=torch.int64)\n            target['masks'] = torch.zeros((0, self.img_size[1], self.img_size[0]), dtype=torch.uint8)\n            target['area'] = torch.tensor([], dtype=torch.float32)\n            target['iscrowd'] = torch.tensor([], dtype=torch.int64)\n        else:\n            masks_t = torch.as_tensor(np.stack(masks, axis=0), dtype=torch.uint8)  # (N,H,W)\n            boxes_t = torch.as_tensor(boxes, dtype=torch.float32)  # (N,4)\n            labels_t = torch.ones((masks_t.shape[0],), dtype=torch.int64)\n            areas = ((boxes_t[:,3] - boxes_t[:,1]) * (boxes_t[:,2] - boxes_t[:,0])).to(torch.float32)\n            iscrowd = torch.zeros((masks_t.shape[0],), dtype=torch.int64)\n            target['boxes'] = boxes_t\n            target['labels'] = labels_t\n            target['masks'] = masks_t\n            target['area'] = areas\n            target['iscrowd'] = iscrowd\n\n        target['image_id'] = torch.tensor([idx])\n        return image, target\n\ndef collate_fn(batch):\n    images, targets = list(zip(*batch))\n    return list(images), list(targets)\n\n# build datasets + loaders\ntrain_ds = MaskRCNNDataset(DATA_ROOT, 'train', input_mode=INPUT_MODE, img_size=IMG_SIZE)\nval_ds   = MaskRCNNDataset(DATA_ROOT, 'val',   input_mode=INPUT_MODE, img_size=IMG_SIZE)\ntest_ds  = MaskRCNNDataset(DATA_ROOT, 'test',  input_mode=INPUT_MODE, img_size=IMG_SIZE)\n\ntrain_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\nval_loader   = DataLoader(val_ds,   batch_size=VAL_BATCH,   shuffle=False, num_workers=1, collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds,  batch_size=TEST_BATCH,  shuffle=False, num_workers=1, collate_fn=collate_fn)\n\nprint(f\"Train {len(train_ds)} | Val {len(val_ds)} | Test {len(test_ds)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 5: Custom CNN backbone + FPN wrapper (no ResNet) ===\n# We'll make a small ResNet-like stack producing 3 feature maps (C3, C4, C5),\n# then pass them through torchvision.ops.FeaturePyramidNetwork to produce FPN outputs.\n\nclass ConvBlockBN(nn.Module):\n    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1, dropout=0.0):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel, stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n        nn.init.kaiming_normal_(self.conv.weight, mode='fan_out', nonlinearity='relu')\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        return x\n\nclass CustomBackboneWithFPN(nn.Module):\n    def __init__(self, in_channels=3, base_filters=(64,128,256), fpn_out_channels=256, dropout_map=None):\n        super().__init__()\n        # dropout_map: dict for blocks e.g. {'c1':0.1,'c2':0.2,'c3':0.3}\n        if dropout_map is None:\n            dropout_map = {'c1':0.15, 'c2':0.25, 'c3':0.35, 'fpn':0.25}\n        # bottom-up convs producing 3 levels\n        self.stage1 = nn.Sequential(\n            ConvBlockBN(in_channels, base_filters[0], dropout=dropout_map.get('c1',0.0)),\n            ConvBlockBN(base_filters[0], base_filters[0], dropout=dropout_map.get('c1',0.0))\n        )\n        self.pool1 = nn.MaxPool2d(2)  # /2\n        self.stage2 = nn.Sequential(\n            ConvBlockBN(base_filters[0], base_filters[1], dropout=dropout_map.get('c2',0.0)),\n            ConvBlockBN(base_filters[1], base_filters[1], dropout=dropout_map.get('c2',0.0))\n        )\n        self.pool2 = nn.MaxPool2d(2)  # /4\n        self.stage3 = nn.Sequential(\n            ConvBlockBN(base_filters[1], base_filters[2], dropout=dropout_map.get('c3',0.0)),\n            ConvBlockBN(base_filters[2], base_filters[2], dropout=dropout_map.get('c3',0.0))\n        )\n        # produce feature maps C3, C4, C5 (we'll use stage1 output as C3 for simplicity)\n        # create FPN\n        in_channels_list = [base_filters[0], base_filters[1], base_filters[2]]\n        self.fpn = FeaturePyramidNetwork(in_channels_list=in_channels_list, out_channels=fpn_out_channels)\n        # optionally wrap fpn convs with dropout\n        self.fpn_dropout = nn.Dropout2d(dropout_map.get('fpn', 0.0)) if dropout_map.get('fpn', 0.0) > 0 else nn.Identity()\n        self.out_channels = fpn_out_channels\n\n    def forward(self, x):\n        c1 = self.stage1(x)      # (B, C1, H, W)\n        p1 = self.pool1(c1)\n        c2 = self.stage2(p1)     # (B, C2, H/2, W/2)\n        p2 = self.pool2(c2)\n        c3 = self.stage3(p2)     # (B, C3, H/4, W/4)\n        # Build OrderedDict of feature maps expected by FPN\n        feats = OrderedDict()\n        feats['0'] = c1\n        feats['1'] = c2\n        feats['2'] = c3\n        fpn_outs = self.fpn(feats)  # OrderedDict of p3,p4,p5...\n        # apply dropout to each fpn output\n        for k in list(fpn_outs.keys()):\n            fpn_outs[k] = self.fpn_dropout(fpn_outs[k])\n        return fpn_outs\n\n# Map your very_aggressive_dropout to backbone levels\nvery_aggressive_dropout = {\n    'encoder': [0.0, 0.15, 0.25, 0.4, 0.6],\n    'decoder': [0.6, 0.5, 0.4, 0.3],\n    'skip': 0.4,\n    'final': 0.4\n}\ndrop_map = {'c1': 0.15, 'c2':0.25, 'c3':0.4, 'fpn':0.35}\n\nbackbone = CustomBackboneWithFPN(in_channels=3, base_filters=(64,128,256), fpn_out_channels=256, dropout_map=drop_map)\nbackbone.to(DEVICE)\n\n# quick param count for backbone+fpn\nbp_params = sum(p.numel() for p in backbone.parameters())\nprint(\"Backbone+FPN params:\", f\"{bp_params:,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 6: Build Mask R-CNN using our backbone (make anchors, RoIAlign, and mask/box head customizations) ===\n# Anchor generator (scales and aspect ratios)\nanchor_generator = AnchorGenerator(\n    sizes=((32, 64, 128, 256, 512),), \n    aspect_ratios=((0.5, 1.0, 2.0),)\n)\n\n# RoI Align / RoI Pooler\nroi_pooler = MultiScaleRoIAlign(\n    featmap_names=['0','1','2'],  # keys emitted by our backbone's fpn (we used '0','1','2')\n    output_size=7,\n    sampling_ratio=2\n)\n\nmask_roi_pooler = MultiScaleRoIAlign(\n    featmap_names=['0','1','2'],\n    output_size=14,\n    sampling_ratio=2\n)\n\n# Construct MaskRCNN model\nmodel = MaskRCNN(\n    backbone,\n    num_classes=NUM_CLASSES,\n    rpn_anchor_generator=anchor_generator,\n    box_roi_pool=roi_pooler,\n    mask_roi_pool=mask_roi_pooler\n)\n\n# Replace classification box predictor to insert dropout and match representation size (mapping from earlier)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\nclass CustomFastRCNNPredictor(nn.Module):\n    def __init__(self, in_channels, num_classes, dropout_p=0.4):\n        super().__init__()\n        representation_size = 1024\n        self.fc1 = nn.Linear(in_channels, representation_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(dropout_p)\n        self.cls_score = nn.Linear(representation_size, num_classes)\n        self.bbox_pred = nn.Linear(representation_size, num_classes * 4)\n        nn.init.normal_(self.fc1.weight, std=0.01)\n        nn.init.normal_(self.cls_score.weight, std=0.01)\n        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n        nn.init.constant_(self.fc1.bias, 0)\n        nn.init.constant_(self.cls_score.bias, 0)\n        nn.init.constant_(self.bbox_pred.bias, 0)\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        scores = self.cls_score(x)\n        bbox_deltas = self.bbox_pred(x)\n        return scores, bbox_deltas\n\nmodel.roi_heads.box_predictor = CustomFastRCNNPredictor(in_features, NUM_CLASSES, dropout_p=0.4)\n\n# Replace mask predictor: conv -> relu -> dropout -> conv logits\ntry:\n    in_ch_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\nexcept:\n    in_ch_mask = 256\nhidden_dim = 256\nclass CustomMaskPredictor(nn.Module):\n    def __init__(self, in_channels, dim_reduced=hidden_dim, num_classes=NUM_CLASSES, dropout_p=0.4):\n        super().__init__()\n        self.conv5_mask = nn.Conv2d(in_channels, dim_reduced, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(dropout_p)\n        self.mask_fcn_logits = nn.Conv2d(dim_reduced, num_classes, kernel_size=1)\n        for l in [self.conv5_mask, self.mask_fcn_logits]:\n            nn.init.kaiming_normal_(l.weight, mode='fan_out', nonlinearity='relu')\n            if l.bias is not None:\n                nn.init.constant_(l.bias, 0)\n    def forward(self, x):\n        x = self.conv5_mask(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.mask_fcn_logits(x)\n        return x\n\nmodel.roi_heads.mask_predictor = CustomMaskPredictor(in_ch_mask, dim_reduced=hidden_dim, num_classes=NUM_CLASSES, dropout_p=0.4)\n\n# Move model to device\nmodel.to(DEVICE)\n\n# Parameter count and approximate gfloPs profiling for backbone wrapper (thop)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(\"Total Mask R-CNN params:\", f\"{total_params:,}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 7: Optimizer, scheduler, early stopping (same as UNet++) ===\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6)\nearly_stop = EarlyStopping(patience=10, min_delta=1e-4, min_epochs=10)\n\nEPOCHS = 200\nhistory = []\nbest_val_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    model.train()\n    epoch_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Train\", leave=False)\n    for images, targets in pbar:\n        images = [img.to(DEVICE) for img in images]\n        targets = [{k:v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k,v in t.items()} for t in targets]\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        optimizer.zero_grad()\n        losses.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        epoch_loss += losses.item() * len(images)\n        pbar.set_postfix(dict(loss=losses.item()))\n    epoch_loss = epoch_loss / len(train_ds)\n\n    # Validation: compute losses and predictions for metrics\n    model.eval()\n    val_loss = 0.0\n    outputs_for_metrics = []\n    targets_for_metrics = []\n    with torch.no_grad():\n        for images, targets in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Val\", leave=False):\n            images = [img.to(DEVICE) for img in images]\n            targets = [{k:v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k,v in t.items()} for t in targets]\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            val_loss += losses.item() * len(images)\n\n            preds = model(images)  # inference mode -> list of dicts\n            preds_cpu = [{k:v.detach().cpu() if isinstance(v, torch.Tensor) else v for k,v in p.items()} for p in preds]\n            targets_cpu = [{k:v.detach().cpu() if isinstance(v, torch.Tensor) else v for k,v in t.items()} for t in targets]\n            outputs_for_metrics.extend(preds_cpu)\n            targets_for_metrics.extend(targets_cpu)\n\n    val_loss = val_loss / len(val_ds)\n    mets = batch_metrics_from_outputs(outputs_for_metrics, targets_for_metrics, score_thresh=0.5, mask_thresh=0.3)\n\n    scheduler.step(val_loss)\n    history.append(dict(epoch=epoch+1, train_loss=epoch_loss, val_loss=val_loss,\n                        IoU=mets['iou'], Dice=mets['dice'], Precision=mets['precision'],\n                        Recall=mets['recall'], F1=mets['f1'], Accuracy=mets['acc']))\n\n    print(f\"Epoch {epoch+1}: TL {epoch_loss:.4f} VL {val_loss:.4f} IoU {mets['iou']:.4f} Dice {mets['dice']:.4f} F1 {mets['f1']:.4f} LR {optimizer.param_groups[0]['lr']:.2e}\")\n\n    # save best\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_maskrcnn_custombackbone.pth')\n\n    if early_stop(epoch, val_loss, model):\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n\n# Save training history\npd.DataFrame(history).to_csv('training_history_maskrcnn_custombackbone.csv', index=False)\nprint(\"Training finished.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 8: Test evaluation (load best, compute metrics, save predictions) ===\n# reload best\nmodel.load_state_dict(torch.load('best_maskrcnn_custombackbone.pth', map_location=DEVICE))\nmodel.eval()\n\nagg = dict(tp=0,fp=0,fn=0,tn=0)\nall_preds_semantic = []\nwith torch.no_grad():\n    for images, targets in tqdm(test_loader, desc=\"Test\", leave=False):\n        images = [img.to(DEVICE) for img in images]\n        targets_cpu = [{k:v for k,v in t.items()} for t in targets]  # keep on CPU for metric fn\n        preds = model(images)\n        preds_cpu = [{k:v.detach().cpu() if isinstance(v, torch.Tensor) else v for k,v in p.items()} for p in preds]\n        mets = batch_metrics_from_outputs(preds_cpu, targets_cpu, score_thresh=0.5, mask_thresh=0.3)\n        for k in agg: agg[k] += mets[k]\n        for p in preds_cpu:\n            H, W = IMG_SIZE[1], IMG_SIZE[0]\n            pred_sem = _to_binary_semantic_from_instances(p, (H,W), score_thresh=0.5, mask_thresh=0.3)\n            all_preds_semantic.append(pred_sem.unsqueeze(0))\n\nif len(all_preds_semantic) > 0:\n    all_preds_tensor = torch.cat(all_preds_semantic, dim=0)\nelse:\n    all_preds_tensor = torch.zeros((0, IMG_SIZE[1], IMG_SIZE[0]))\n\ntp,fp,fn,tn = agg['tp'],agg['fp'],agg['fn'],agg['tn']\neps=1e-8\niou = tp/(tp+fp+fn+eps)\ndice = (2*tp)/(2*tp+fp+fn+eps)\nprecision = tp/(tp+fp+eps) if (tp+fp)>0 else 0\nrecall = tp/(tp+fn+eps) if (tp+fn)>0 else 0\nf1 = 2*precision*recall/(precision+recall+eps) if (precision+recall)>0 else 0\nacc = (tp+tn)/(tp+tn+fp+fn+eps)\n\ncm = np.array([[tn, fp],[fn, tp]])\nmetrics = dict(IoU=iou, Dice=dice, Precision=precision, Recall=recall, F1=f1, Accuracy=acc, TP=tp, FP=fp, FN=fn, TN=tn)\n\nprint(\"\\nTest Metrics (Mask R-CNN custom backbone):\")\nprint(f\"IoU: {iou:.4f}, Dice: {dice:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n\npd.DataFrame([metrics]).to_csv('test_metrics_maskrcnn_custombackbone.csv', index=False)\nnp.savetxt('confusion_matrix_maskrcnn_custombackbone.txt', cm, fmt='%d')\n\nos.makedirs('test_predictions_maskrcnn_custombackbone', exist_ok=True)\nfor i in range(min(10, all_preds_tensor.shape[0])):\n    img = (all_preds_tensor[i].numpy()*255).astype('uint8')\n    Image.fromarray(img).save(f'test_predictions_maskrcnn_custombackbone/pred_{i}.png')\nprint(\"Saved some test prediction masks.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 9: Visualization (training curves) ===\nhist_df = pd.read_csv('training_history_maskrcnn_custombackbone.csv')\nfig, ((ax1, ax2, ax3),(ax4, ax5, ax6)) = plt.subplots(2,3, figsize=(16,8))\n\nax1.plot(hist_df['epoch'], hist_df['train_loss'], label='Train Loss'); ax1.plot(hist_df['epoch'], hist_df['val_loss'], label='Val Loss'); ax1.set_title('Loss Curves'); ax1.legend(); ax1.grid(True, alpha=0.3)\nax2.plot(hist_df['epoch'], hist_df['IoU']); ax2.set_title('Validation IoU'); ax3.plot(hist_df['epoch'], hist_df['Dice']); ax3.set_title('Validation Dice')\nax4.plot(hist_df['epoch'], hist_df['Precision'], label='Precision'); ax4.plot(hist_df['epoch'], hist_df['Recall'], label='Recall'); ax4.legend(); ax4.set_title('Precision & Recall')\nax5.plot(hist_df['epoch'], hist_df['F1'], label='F1'); ax5.plot(hist_df['epoch'], hist_df['Accuracy'], label='Accuracy'); ax5.legend(); ax5.set_title('F1 & Accuracy')\nax6.axis('off')\nif len(hist_df)>0:\n    best_epoch = hist_df.loc[hist_df['val_loss'].idxmin(), 'epoch']\n    best_val = hist_df['val_loss'].min()\n    best_iou = hist_df['IoU'].max()\n    best_dice = hist_df['Dice'].max()\n    summary_text = f\"Total epochs: {len(hist_df)}\\\\nBest Epoch: {best_epoch}\\\\nVal Loss: {best_val:.4f}\\\\nIoU: {best_iou:.4f}\\\\nDice: {best_dice:.4f}\"\n    ax6.text(0.05, 0.5, summary_text, fontsize=10, fontfamily='monospace', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\"))\n\nplt.tight_layout()\nplt.savefig('training_curves_maskrcnn_custombackbone.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Visualization saved.\")\n\nprint(\"Total model params:\", f\"{total_params:,}\")\nif gflops is not None:\n    print(f\"Approx backbone+FPN GFLOPs for input {IMG_SIZE[1]}x{IMG_SIZE[0]}: {gflops:.3f} GFLOPs (approx).\")\n    print(\"Total model GFLOPs (including RPN & RoIHeads) will be higher and depends on proposals; this is an approximation for backbone+FPN.\")\nelse:\n    print(\"GFLOPs not available (profiling failed).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}