{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12771533,"sourceType":"datasetVersion","datasetId":8073899}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b17352e8","cell_type":"markdown","source":"# SegFormer Change Detection Training Notebook\nThis notebook trains a SegFormer model for change detection using A/B/label folders for train, val, and test. It implements SegFormer from scratch with the same aggressive dropout strategy, training approach, and evaluation metrics as the U-Net++ implementation.","metadata":{}},{"id":"fba7fbc6","cell_type":"markdown","source":"## Assignment Compliance (Segmentation)\n- Problem: Change detection (binary segmentation of change mask)\n- Model: SegFormer (Vision Transformer-based segmentation model from scratch)\n- Epochs: Min 50 with early stopping (patience 10)\n- Data: Using existing train / val / test folders exactly as provided (no re-splitting enforced).\n- Metrics tracked: IoU, Dice, Precision, Recall, F1, Accuracy, Loss + confusion matrix (pixel-wise)\n- Outputs: Metric plots, sample predictions, parameter count, saved best weights.\n- Saved artifacts: best_segformer.pth, training_history_segformer.csv, test_metrics_segformer.csv, confusion_matrix_segformer.txt, prediction PNGs.\n- Dropout: Very aggressive dropout strategy matching U-Net++ configuration","metadata":{}},{"id":"15ff1d50","cell_type":"code","source":"# Install all required packages\n!pip install torch torchvision timm albumentations scikit-learn pandas tqdm matplotlib seaborn einops --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:59:35.024527Z","iopub.execute_input":"2025-08-18T18:59:35.025270Z","iopub.status.idle":"2025-08-18T18:59:38.411913Z","shell.execute_reply.started":"2025-08-18T18:59:35.025236Z","shell.execute_reply":"2025-08-18T18:59:38.410765Z"}},"outputs":[],"execution_count":8},{"id":"96e3c941","cell_type":"code","source":"# Imports & Setup for custom SegFormer training (from scratch implementation)\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom einops import rearrange\n\n# Device & Reproducibility\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nprint(f\"Using device: {DEVICE}\")\n\n# Loss components (Dice + BCE) - Same as U-Net++\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super().__init__()\n        self.smooth = smooth\n    def forward(self, preds, targets):\n        # preds: probabilities after sigmoid, targets: binary\n        preds = preds.contiguous()\n        targets = targets.contiguous()\n        intersection = (preds * targets).sum(dim=(2,3))\n        denom = preds.sum(dim=(2,3)) + targets.sum(dim=(2,3))\n        dice = (2 * intersection + self.smooth) / (denom + self.smooth)\n        return 1 - dice.mean()\n\ndef combined_loss(logits, targets, bce_w=0.6, dice_w=0.4):\n    bce = nn.BCEWithLogitsLoss()(logits, targets)\n    probs = torch.sigmoid(logits)\n    dloss = DiceLoss()(probs, targets)\n    return bce_w * bce + dice_w * dloss\n\n@torch.no_grad()\ndef batch_metrics(logits, targets, thresh=0.3):  # Same 0.3 threshold as U-Net++\n    probs = torch.sigmoid(logits)\n    preds = (probs >= thresh).float()\n    p = preds.view(-1).cpu().numpy()\n    t = targets.view(-1).cpu().numpy()\n    # Confusion components\n    cm = confusion_matrix(t, p, labels=[0,1]) if (t.sum()>0 or p.sum()>0) else np.array([[len(t),0],[0,0]])\n    if cm.shape == (2,2):\n        tn, fp, fn, tp = cm.ravel()\n    else:  # degenerate\n        tn = fp = fn = tp = 0\n    eps = 1e-8\n    iou = tp / (tp + fp + fn + eps)\n    dice = (2*tp) / (2*tp + fp + fn + eps)\n    precision = tp / (tp + fp + eps) if (tp+fp)>0 else 0.0\n    recall = tp / (tp + fn + eps) if (tp+fn)>0 else 0.0\n    f1 = 2*precision*recall/(precision+recall+eps) if (precision+recall)>0 else 0.0\n    acc = (tp + tn) / (tp + tn + fp + fn + eps)\n    return dict(tp=int(tp), fp=int(fp), fn=int(fn), tn=int(tn), iou=float(iou), dice=float(dice), precision=float(precision), recall=float(recall), f1=float(f1), acc=float(acc))\n\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=1e-4, restore_best=True, min_epochs=10):  # Same patience as U-Net++\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best = restore_best\n        self.best_loss = None\n        self.counter = 0\n        self.best_state = None\n        self.min_epochs = min_epochs\n    def __call__(self, epoch, current_loss, model):\n        if self.best_loss is None or (self.best_loss - current_loss) > self.min_delta:\n            self.best_loss = current_loss\n            self.counter = 0\n            if self.restore_best:\n                self.best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n        else:\n            self.counter += 1\n        if epoch+1 < self.min_epochs:\n            return False\n        if self.counter >= self.patience:\n            if self.restore_best and self.best_state is not None:\n                model.load_state_dict(self.best_state)\n            return True\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:59:38.413536Z","iopub.execute_input":"2025-08-18T18:59:38.413845Z","iopub.status.idle":"2025-08-18T18:59:38.987072Z","shell.execute_reply.started":"2025-08-18T18:59:38.413819Z","shell.execute_reply":"2025-08-18T18:59:38.986343Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"id":"fc2bb315","cell_type":"code","source":"# Dataset (same as U-Net++ implementation)\nDATA_ROOT = '/kaggle/input/earthquakedatasetnew/earthquakeDataset'  # Adjust to local path as needed\nIMG_SIZE = (256, 256)\nTRAIN_BATCH = 6  # Same batch sizes as U-Net++\nVAL_BATCH = 2\nTEST_BATCH = 1\n\ntransform_img = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\ntransform_mask = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor()\n])\n\nclass ChangeDataset(Dataset):\n    def __init__(self, root, split='train'):\n        if split=='train':\n            a_dir = os.path.join(root,'train','A_train_aug')\n            b_dir = os.path.join(root,'train','B_train_aug')\n            m_dir = os.path.join(root,'train','label_train_aug')\n        elif split=='val':\n            a_dir = os.path.join(root,'val','A_val')\n            b_dir = os.path.join(root,'val','B_val')\n            m_dir = os.path.join(root,'val','label_val')\n        else:\n            a_dir = os.path.join(root,'test','A_test')\n            b_dir = os.path.join(root,'test','B_test')\n            m_dir = os.path.join(root,'test','label_test')\n        self.a_files = sorted([f for f in os.listdir(a_dir) if f.endswith('.png')])\n        self.a_dir, self.b_dir, self.m_dir = a_dir, b_dir, m_dir\n    def __len__(self): return len(self.a_files)\n    def __getitem__(self, idx):\n        name = self.a_files[idx]\n        a = Image.open(os.path.join(self.a_dir,name)).convert('RGB')\n        b = Image.open(os.path.join(self.b_dir,name)).convert('RGB')\n        m = Image.open(os.path.join(self.m_dir,name)).convert('L')\n        a = transform_img(a)\n        b = transform_img(b)\n        m = transform_mask(m)\n        m = (m>0.5).float()\n        x = torch.cat([a,b], dim=0)  # 6 channels (A+B concatenated)\n        return x, m\n\ntrain_ds = ChangeDataset(DATA_ROOT,'train')\nval_ds = ChangeDataset(DATA_ROOT,'val')\ntest_ds = ChangeDataset(DATA_ROOT,'test')\n\ntrain_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=VAL_BATCH, shuffle=False, num_workers=1)\ntest_loader = DataLoader(test_ds, batch_size=TEST_BATCH, shuffle=False, num_workers=1)\n\nprint(f\"Train {len(train_ds)} | Val {len(val_ds)} | Test {len(test_ds)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:59:38.987733Z","iopub.execute_input":"2025-08-18T18:59:38.987923Z","iopub.status.idle":"2025-08-18T18:59:39.016888Z","shell.execute_reply.started":"2025-08-18T18:59:38.987908Z","shell.execute_reply":"2025-08-18T18:59:39.016177Z"}},"outputs":[{"name":"stdout","text":"Train 2268 | Val 189 | Test 189\n","output_type":"stream"}],"execution_count":10},{"id":"7c9ce27a","cell_type":"code","source":"# Custom SegFormer implementation from scratch with aggressive dropout\nclass PatchEmbed(nn.Module):\n    \"\"\"Overlap Patch Embedding\"\"\"\n    def __init__(self, img_size=256, patch_size=7, stride=4, in_chans=6, embed_dim=64, dropout_rate=0.0):\n        super().__init__()\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride, padding=patch_size//2)\n        self.norm = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n        \n    def forward(self, x):\n        x = self.proj(x)  # (B, C, H, W)\n        B, C, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)  # (B, H*W, C)\n        x = self.norm(x)\n        x = self.dropout(x)\n        return x, H, W\n\nclass EfficientSelfAttention(nn.Module):\n    \"\"\"Efficient Self-Attention with reduction ratio\"\"\"\n    def __init__(self, dim, num_heads=8, qkv_bias=False, sr_ratio=1, dropout_rate=0.0, attention_dropout=0.0):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n        self.sr_ratio = sr_ratio\n        \n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        \n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n        \n        self.attn_drop = nn.Dropout(attention_dropout)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout_rate)\n        \n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n        \n        if self.sr_ratio > 1:\n            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n            x_ = self.norm(x_)\n            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        else:\n            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        k, v = kv[0], kv[1]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass MixFFN(nn.Module):\n    \"\"\"Mix-FFN with depthwise convolution\"\"\"\n    def __init__(self, dim, hidden_dim, dropout_rate=0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.dwconv = nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1, groups=hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.drop = nn.Dropout(dropout_rate)\n        \n    def forward(self, x, H, W):\n        x = self.fc1(x)\n        B, N, C = x.shape\n        x = x.permute(0, 2, 1).reshape(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).permute(0, 2, 1)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer Block with efficient attention and Mix-FFN\"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, sr_ratio=1, \n                 dropout_rate=0.0, attention_dropout=0.0, layer_dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = EfficientSelfAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, \n                                         sr_ratio=sr_ratio, dropout_rate=dropout_rate, \n                                         attention_dropout=attention_dropout)\n        self.layer_drop1 = nn.Dropout(layer_dropout) if layer_dropout > 0 else nn.Identity()\n        \n        self.norm2 = nn.LayerNorm(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = MixFFN(dim, mlp_hidden_dim, dropout_rate=dropout_rate)\n        self.layer_drop2 = nn.Dropout(layer_dropout) if layer_dropout > 0 else nn.Identity()\n        \n    def forward(self, x, H, W):\n        x = x + self.layer_drop1(self.attn(self.norm1(x), H, W))\n        x = x + self.layer_drop2(self.mlp(self.norm2(x), H, W))\n        return x\n\nclass MixTransformerStage(nn.Module):\n    \"\"\"Mix Transformer Stage\"\"\"\n    def __init__(self, img_size, patch_size, stride, in_chans, embed_dim, depth, num_heads, \n                 mlp_ratio, qkv_bias, sr_ratio, dropout_rates):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbed(img_size, patch_size, stride, in_chans, embed_dim, \n                                    dropout_rates.get('patch_embed', 0.0))\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, qkv_bias, sr_ratio, \n                           dropout_rates.get('transformer', 0.0),\n                           dropout_rates.get('attention', 0.0),\n                           dropout_rates.get('layer', 0.0))\n            for _ in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def forward(self, x):\n        x, H, W = self.patch_embed(x)\n        for blk in self.blocks:\n            x = blk(x, H, W)\n        x = self.norm(x)\n        x = x.reshape(-1, H, W, x.size(-1)).permute(0, 3, 1, 2)\n        return x\n\nclass MLPDecoder(nn.Module):\n    \"\"\"Lightweight MLP Decoder\"\"\"\n    def __init__(self, in_channels, num_classes=1, dropout_rate=0.0):\n        super().__init__()\n        self.decoder = nn.Sequential(\n            nn.Conv2d(sum(in_channels), 256, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(dropout_rate),\n            nn.Conv2d(256, num_classes, 1)\n        )\n        \n    def forward(self, features):\n        # Upsample all features to the same size (1/4 of input)\n        target_size = features[0].shape[2:]\n        upsampled = []\n        for feat in features:\n            if feat.shape[2:] != target_size:\n                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n            upsampled.append(feat)\n        \n        # Concatenate and decode\n        x = torch.cat(upsampled, dim=1)\n        x = self.decoder(x)\n        return x\n\nclass SegFormer(nn.Module):\n    \"\"\"SegFormer model from scratch with aggressive dropout\"\"\"\n    def __init__(self, img_size=256, in_chans=6, num_classes=1, \n                 embed_dims=[64, 128, 256, 512], depths=[3, 4, 6, 3], num_heads=[1, 2, 4, 8],\n                 mlp_ratios=[4, 4, 4, 4], qkv_bias=True, sr_ratios=[8, 4, 2, 1], dropout_rates=None):\n        super().__init__()\n        \n        # Same aggressive dropout as U-Net++\n        if dropout_rates is None:\n            dropout_rates = {\n                'patch_embed': [0.0, 0.15, 0.25, 0.4],     # Increasing dropout\n                'transformer': [0.0, 0.2, 0.35, 0.5],     # Very high dropout in deeper stages\n                'attention': [0.0, 0.15, 0.3, 0.45],       # Attention dropout\n                'layer': [0.0, 0.1, 0.2, 0.35],           # Layer dropout (stochastic depth)\n                'decoder': 0.4                             # High dropout in decoder\n            }\n        \n        # Multi-stage encoder\n        self.stages = nn.ModuleList()\n        patch_sizes = [7, 3, 3, 3]\n        strides = [4, 2, 2, 2]\n        \n        for i in range(len(embed_dims)):\n            stage_dropout = {\n                'patch_embed': dropout_rates['patch_embed'][i],\n                'transformer': dropout_rates['transformer'][i],\n                'attention': dropout_rates['attention'][i],\n                'layer': dropout_rates['layer'][i]\n            }\n            \n            stage = MixTransformerStage(\n                img_size=img_size // (4 * 2**i) if i > 0 else img_size,\n                patch_size=patch_sizes[i],\n                stride=strides[i],\n                in_chans=in_chans if i == 0 else embed_dims[i-1],\n                embed_dim=embed_dims[i],\n                depth=depths[i],\n                num_heads=num_heads[i],\n                mlp_ratio=mlp_ratios[i],\n                qkv_bias=qkv_bias,\n                sr_ratio=sr_ratios[i],\n                dropout_rates=stage_dropout\n            )\n            self.stages.append(stage)\n        \n        # Lightweight MLP decoder with dropout\n        self.decoder = MLPDecoder(embed_dims, num_classes, dropout_rates['decoder'])\n        \n    def forward(self, x):\n        # Multi-scale feature extraction\n        features = []\n        for stage in self.stages:\n            x = stage(x)\n            features.append(x)\n        \n        # Decode features\n        x = self.decoder(features)\n        \n        # Upsample to input resolution\n        x = F.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False)\n        \n        return x\n\n# Very aggressive dropout configuration (matching U-Net++)\nvery_aggressive_dropout = {\n    'patch_embed': [0.0, 0.15, 0.25, 0.4],     # Increasing dropout in deeper stages\n    'transformer': [0.0, 0.2, 0.4, 0.6],       # Very high dropout\n    'attention': [0.0, 0.15, 0.3, 0.5],        # High attention dropout\n    'layer': [0.0, 0.1, 0.25, 0.4],           # High stochastic depth\n    'decoder': 0.4                             # High decoder dropout\n}\n\n# Create SegFormer model with aggressive dropout\nmodel = SegFormer(\n    img_size=256, \n    in_chans=6, \n    num_classes=1,\n    embed_dims=[64, 128, 256, 512],  # Similar capacity to U-Net++\n    depths=[3, 4, 6, 3],             # Reasonable depth\n    num_heads=[1, 2, 4, 8],\n    dropout_rates=very_aggressive_dropout\n).to(DEVICE)\n\nprint(f\"Model params: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Same optimizer settings as U-Net++\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6)\nearly_stop = EarlyStopping(patience=10, min_delta=1e-4, min_epochs=10)  # Same as U-Net++\n\nprint(\"SegFormer model created with aggressive dropout strategy!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:59:39.018543Z","iopub.execute_input":"2025-08-18T18:59:39.019004Z","iopub.status.idle":"2025-08-18T18:59:39.520128Z","shell.execute_reply.started":"2025-08-18T18:59:39.018987Z","shell.execute_reply":"2025-08-18T18:59:39.519446Z"}},"outputs":[{"name":"stdout","text":"Model params: 20,522,817\nSegFormer model created with aggressive dropout strategy!\n","output_type":"stream"}],"execution_count":11},{"id":"6171500d","cell_type":"code","source":"# Training Loop - Same configuration as U-Net++\nEPOCHS = 200\nhistory = []\n\nfor epoch in range(EPOCHS):\n    model.train()  # Important: enables dropout\n    train_loss = 0.0\n    for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Train\", leave=False):\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        optimizer.zero_grad()\n        logits = model(xb)\n        loss = combined_loss(logits, yb)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n        optimizer.step()\n        train_loss += loss.item() * xb.size(0)\n    train_loss /= len(train_loader.dataset)\n\n    # Validation\n    model.eval()  # Important: disables dropout\n    val_loss = 0.0\n    agg = dict(tp=0,fp=0,fn=0,tn=0)\n    with torch.no_grad():\n        for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} Val\", leave=False):\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            logits = model(xb)\n            loss = combined_loss(logits, yb)\n            val_loss += loss.item() * xb.size(0)\n            mets = batch_metrics(logits, yb, thresh=0.3)  # Same 0.3 threshold\n            for k in agg: agg[k] += mets[k]\n    \n    val_loss /= len(val_loader.dataset)\n    eps=1e-8\n    tp,fp,fn,tn = agg['tp'],agg['fp'],agg['fn'],agg['tn']\n    iou = tp / (tp+fp+fn+eps)\n    dice = (2*tp)/(2*tp+fp+fn+eps)\n    precision = tp/(tp+fp+eps) if (tp+fp)>0 else 0\n    recall = tp/(tp+fn+eps) if (tp+fn)>0 else 0\n    f1 = 2*precision*recall/(precision+recall+eps) if (precision+recall)>0 else 0\n    acc = (tp+tn)/(tp+tn+fp+fn+eps)\n    history.append(dict(epoch=epoch+1, train_loss=train_loss, val_loss=val_loss, IoU=iou, Dice=dice, Precision=precision, Recall=recall, F1=f1, Accuracy=acc))\n\n    scheduler.step(val_loss)\n    print(f\"Epoch {epoch+1}: TL {train_loss:.4f} VL {val_loss:.4f} IoU {iou:.4f} Dice {dice:.4f} F1 {f1:.4f} LR {optimizer.param_groups[0]['lr']:.2e}\")\n\n    # Save best model\n    if epoch==0 or val_loss == min(h['val_loss'] for h in history):\n        torch.save(model.state_dict(), 'best_segformer.pth')\n\n    if early_stop(epoch, val_loss, model):\n        print(f\"Early stopping at epoch {epoch+1}\")\n        break\n\n# Save training history\npd.DataFrame(history).to_csv('training_history_segformer.csv', index=False)\nprint('SegFormer training complete.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:59:39.520814Z","iopub.execute_input":"2025-08-18T18:59:39.521056Z","execution_failed":"2025-08-18T19:04:21.614Z"}},"outputs":[{"name":"stderr","text":"                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: TL 0.5719 VL 0.4734 IoU 0.2640 Dice 0.4178 F1 0.4178 LR 1.00e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/200 Train:  24%|██▍       | 90/378 [00:50<02:42,  1.77it/s]","output_type":"stream"}],"execution_count":null},{"id":"ed199445","cell_type":"code","source":"# Test evaluation - Same as U-Net++\nmodel = SegFormer(\n    img_size=256, \n    in_chans=6, \n    num_classes=1,\n    embed_dims=[64, 128, 256, 512],\n    depths=[3, 4, 6, 3],\n    num_heads=[1, 2, 4, 8],\n    dropout_rates=very_aggressive_dropout\n).to(DEVICE)\n\n# Load the trained weights\nmodel.load_state_dict(torch.load('best_segformer.pth', map_location=DEVICE))\nmodel.eval()  # IMPORTANT: This disables dropout for inference\n\nagg = dict(tp=0,fp=0,fn=0,tn=0)\nall_preds = []\n\nwith torch.no_grad():\n    for xb, yb in tqdm(test_loader, desc=\"Test\", leave=False):\n        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n        logits = model(xb)\n        mets = batch_metrics(logits, yb, thresh=0.3)  # Same 0.3 threshold\n        for k in agg: agg[k] += mets[k]\n        probs = torch.sigmoid(logits)\n        preds = (probs>=0.3).float().cpu()  # Same 0.3 threshold\n        all_preds.append(preds)\n\nall_preds = torch.cat(all_preds, dim=0)\neps=1e-8\ntp,fp,fn,tn = agg['tp'],agg['fp'],agg['fn'],agg['tn']\niou = tp/(tp+fp+fn+eps)\ndice = (2*tp)/(2*tp+fp+fn+eps)\nprecision = tp/(tp+fp+eps) if (tp+fp)>0 else 0\nrecall = tp/(tp+fn+eps) if (tp+fn)>0 else 0\nf1 = 2*precision*recall/(precision+recall+eps) if (precision+recall)>0 else 0\nacc = (tp+tn)/(tp+tn+fp+fn+eps)\n\ncm = np.array([[tn, fp],[fn, tp]])\nmetrics = dict(IoU=iou, Dice=dice, Precision=precision, Recall=recall, F1=f1, Accuracy=acc, TP=tp, FP=fp, FN=fn, TN=tn)\n\nprint('\\nTest Metrics (SegFormer with Aggressive Dropout):')\nprint(f'IoU: {iou:.4f}')\nprint(f'Dice: {dice:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1: {f1:.4f}')\nprint(f'Accuracy: {acc:.4f}')\n\n# Save results\npd.DataFrame([metrics]).to_csv('test_metrics_segformer.csv', index=False)\nnp.savetxt('confusion_matrix_segformer.txt', cm, fmt='%d')\n\n# Save first 10 prediction masks\nos.makedirs('test_predictions_segformer', exist_ok=True)\nfor i in range(min(10, all_preds.shape[0])):\n    img = (all_preds[i,0].numpy()*255).astype('uint8')\n    Image.fromarray(img).save(f'test_predictions_segformer/pred_{i}.png')\nprint('Saved SegFormer prediction samples.')\nprint(\"SegFormer testing complete with 0.3 threshold!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-18T19:04:21.615Z"}},"outputs":[],"execution_count":null},{"id":"c48611ee","cell_type":"code","source":"# Visualization - Same as U-Net++\nhist_df = pd.read_csv('training_history_segformer.csv')\nprint('SegFormer Training History:')\nprint(hist_df.head())\n\n# Create training plots\nfig, ((ax1, ax2, ax3),(ax4, ax5, ax6)) = plt.subplots(2,3, figsize=(16,8))\n\n# Plot 1: Loss curves\nax1.plot(hist_df['epoch'], hist_df['train_loss'], label='Train Loss', color='blue')\nax1.plot(hist_df['epoch'], hist_df['val_loss'], label='Val Loss', color='red')\nax1.set_title('SegFormer Loss Curves')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: IoU\nax2.plot(hist_df['epoch'], hist_df['IoU'], label='IoU', color='green')\nax2.set_title('SegFormer Validation IoU')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('IoU')\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Dice\nax3.plot(hist_df['epoch'], hist_df['Dice'], label='Dice', color='orange')\nax3.set_title('SegFormer Validation Dice')\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Dice')\nax3.grid(True, alpha=0.3)\n\n# Plot 4: Precision & Recall\nax4.plot(hist_df['epoch'], hist_df['Precision'], label='Precision', color='purple')\nax4.plot(hist_df['epoch'], hist_df['Recall'], label='Recall', color='brown')\nax4.set_title('SegFormer Precision & Recall')\nax4.set_xlabel('Epoch')\nax4.set_ylabel('Score')\nax4.legend()\nax4.grid(True, alpha=0.3)\n\n# Plot 5: F1 & Accuracy\nax5.plot(hist_df['epoch'], hist_df['F1'], label='F1', color='red')\nax5.plot(hist_df['epoch'], hist_df['Accuracy'], label='Accuracy', color='blue')\nax5.set_title('SegFormer F1 & Accuracy')\nax5.set_xlabel('Epoch')\nax5.set_ylabel('Score')\nax5.legend()\nax5.grid(True, alpha=0.3)\n\n# Plot 6: Summary statistics\nax6.axis('off')\nif len(hist_df) > 0:\n    best_epoch = hist_df.loc[hist_df['val_loss'].idxmin(), 'epoch']\n    best_val_loss = hist_df['val_loss'].min()\n    best_iou = hist_df['IoU'].max()\n    best_dice = hist_df['Dice'].max()\n    best_f1 = hist_df['F1'].max()\n    \n    summary_text = f\"\"\"\n    SEGFORMER TRAINING SUMMARY\n    =========================\n    Total Epochs: {len(hist_df)}\n    Best Epoch: {best_epoch}\n    \n    Best Metrics:\n    Val Loss: {best_val_loss:.4f}\n    IoU: {best_iou:.4f}\n    Dice: {best_dice:.4f}\n    F1: {best_f1:.4f}\n    \n    Final Train/Val Gap:\n    {hist_df['val_loss'].iloc[-1] - hist_df['train_loss'].iloc[-1]:.4f}\n    \n    Model: SegFormer w/ Aggressive Dropout\n    Threshold: 0.3\n    \"\"\"\n    ax6.text(0.1, 0.5, summary_text, fontsize=9, fontfamily='monospace',\n             verticalalignment='center', bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n\nplt.tight_layout()\nplt.savefig('training_curves_segformer.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"SEGFORMER IMPLEMENTATION COMPLETE!\")\nprint(\"=\"*50)\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Training Epochs: {len(hist_df)}\")\nprint(f\"Best Validation IoU: {best_iou:.4f}\")\nprint(f\"Best Validation Dice: {best_dice:.4f}\")\nprint(f\"Best Validation F1: {best_f1:.4f}\")\nprint(\"\\nFiles generated:\")\nprint(\"- best_segformer.pth\")\nprint(\"- training_history_segformer.csv\")\nprint(\"- test_metrics_segformer.csv\")\nprint(\"- confusion_matrix_segformer.txt\")\nprint(\"- training_curves_segformer.png\")\nprint(\"- test_predictions_segformer/ (folder with predictions)\")\nprint(\"\\nSegFormer training and evaluation completed successfully!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-18T19:04:21.615Z"}},"outputs":[],"execution_count":null}]}